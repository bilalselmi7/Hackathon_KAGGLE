---
title: "Hackathon Report -- SELMI Bilal -- FADILI Yanis"
author: "SELMI Bilal, FADILI Yanis"
date: "`r format(Sys.time())`"
output:
  prettydoc::html_pretty:
subtitle: Hackathon

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# STEP 1

## WE LOAD THE LIBRARIES THAT WE NEED FOR THE FUTURE

### (we didn't put them all in first because we didn't know which one we were going to need but every time we needed one we added it here)

```{r eval=TRUE}

library(caTools) # for splitting 
library(randomForest) # for random forests and bagging
library(gbm) # for boosting
library(MLmetrics) # for F1-Score

```

# STEP 2

## WE LOAD THE DATASETS THAT WE WILL WORK WITH


```{r eval=TRUE}

df <- read.csv("C:/Users/SELMI Bilal/Desktop/data.csv") # The one to build our models with
summary(df)

test_final = read.csv("C:/Users/SELMI Bilal/Desktop/test.csv")  # The one from Kaggle to predict  

```

# STEP 3

## WE SPLIT THE DATASET INTO TRAINING AND TESTING SETS IN ORDER TO TRAIN AND TEST OUR PREDICTIVE MODELS


```{r eval=TRUE}

set.seed(704043) # Student ID (Bilal's one)
split = sample.split(df, SplitRatio = 0.75) # 75% of the population in the training set and the 25% other in the test
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)

```

# STEP 4

## WE START TO BUILD OUR PREDICTIVE MODELS FOR THIS CLASSIFICATION PROBLEM

### First, we immediately thought of a logistic regression model
```{r eval=TRUE}

set.seed(704043) 
logistic_model <- glm(y~., family='binomial',data=train)
logistic_model.pred <- predict.glm(logistic_model, newdata=test, type='response')
y_hat = ifelse(logistic_model.pred > 0.5,"yes","no") # the outputs are yes or no


```

## But we had to create a function to test the F1-Score that was going to be evaluated

```{r eval=TRUE}

# The F1_Score function comes from the MLmetrics library

F1_Score(y_pred=y_hat,y_true=test$y,positive = "yes")
F1_Score(y_pred=y_hat,y_true=test$y,positive = "no") # *

# This one is hand self made and calculate the precision and the recall and returns the F1_Score
# by doing 2*(Precision*Recall)/(Precision+Recall)
F1=function(y,prediction)
{
  t=table(y,prediction)
  p=t[1]/(t[1]+t[2])
  r=t[1]/(t[1]+t[3])
  F_1=2*p*r/(p+r)
  return(F_1)
}
F1(test$y,y_hat) # Same result as *

#0.9531388 the result we obtained with this first logistic regression model


# We also build an Accuracy function that We had made in a previous TD to test some more scores in our dataset
ACC = function(a, b){ 
  t = table(a,b)
  print(t)
  accuracy = (t[1]+t[4])/(t[1]+t[2]+t[3]+t[4])
  accuracy
}
ACC(a = y_hat, b = test$y)

```

### Then we decided to test a boosting model, but because we had an error due to the boosting model needed int values only, we had to modify the dataset and change all "yes" values to 1 and all the "no" values to 0 (even if we did not understand this problem because these were Factor types so the fact that it was strings or ints should not influence the comprehension)

```{r eval=TRUE}

train01=train

train01$y = ifelse(train01$y=="yes",1,0)

test01=test

test01$y = ifelse(test01$y=="yes",1,0)

df01 = df

df01$y = ifelse(df01$y=="yes",1,0)

```


# This is with Yanis's seed (we improved a bit the results with his instead of Bilal's)
 We "played" a lot with the parameters in order to improve our F1-Score but it didn't always increased and 
 these were the parameters we had the best results with
 
# We tried as well to train our model in the whole dataframe (df) that we loaded instead of splitting it into a train set which is by this way a subset of the dataset 

```{r eval=TRUE}

set.seed(702928)

boost = gbm(y~., data = train01, distribution = "bernoulli", n.trees = 10000,
            interaction.depth = 6, shrinkage = 0.004, n.minobsinnode = 5) 

boost.pred = predict(boost, newdata = test01, type="response")
boost.hat <- ifelse(boost.pred >0.5,"yes","no")
F1(test$y,boost.hat) # 0.9552388 in R but 0.92900 in Kaggle that we had the 1st place thanks to it
ACC(a = boost.hat, b = test$y)
F1_Score(y_pred=boost.hat,y_true=test$y,positive = "yes")
F1_Score(y_pred=boost.hat,y_true=test$y,positive = "no")

```





## Then, we decided that we were going to work with this model and tried to optimize it, we did a summary in order to visualize all the variables and their relative influences. We then tried to remove them from the datasets but the results were not better...

## We also tried to scale our dataset
           
```{r eval=TRUE}
summary(boost)
#boost = gbm(y~.-default-loan-previous, data = train01, distribution = "bernoulli", n.trees = 10000,
#           interaction.depth = 6, shrinkage = 0.004, n.minobsinnode = 5) 

#df_scaled = scale(df[,-c(21)]) 21 being the number of the outputs so we had to remove it in order no to scale it
#df_scaled = data.frame(df_scaled) then we convert it to dataframe due to a type errors that occured
#df_scaled["target"] = df$target 
```



To finish, we just had to create as many models as we could by changing the parameters, trying to change the dataset, and we finally obtain the results that puts us un the first place with a F1-Score of 0.92900 :)
```{r eval=TRUE}

predictions = predict(boost, newdata = test_final, type="response")
y_pred = ifelse(predictions > 0.5,"yes","no")
table(y_pred)

to_be_submitted = data.frame(id=rownames(test_final), y=y_pred)
write.csv(to_be_submitted , file = "Final_Submission.csv", row.names = F)


```



